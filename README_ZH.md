# CPM.cu

<strong>ä¸­æ–‡ | [English Version](./README.md)</strong>

CPM.cu æ˜¯ä¸€ä¸ªé’ˆå¯¹ç«¯ä¾§å¤§æ¨¡å‹æ¨ç†è®¾è®¡çš„é«˜æ•ˆ CUDA æ¨ç†æ¡†æ¶ï¼Œæ ¸å¿ƒæ”¯æŒ **ç¨€ç–æ¶æ„**ã€**æŠ•æœºé‡‡æ ·** å’Œ **ä½ä½å®½é‡åŒ–** ç­‰å‰æ²¿æŠ€æœ¯åˆ›æ–°ã€‚

<a href="https://github.com/OpenBMB/minicpm"><img src="https://img.shields.io/static/v1?label=MiniCPM4 é¡¹ç›®&message=Web&color=green"></a> &ensp;
<a href="https://github.com/OpenBMB/cpm.cu/blob/main/LICENSE">
  <img alt="GitHub" src="https://img.shields.io/github/license/OpenBMB/cpm.cu">
</a>

<div id="news"></div>

## ğŸ”¥ é¡¹ç›®è¿›å±•

- [2025.06.06] ä¸º [MiniCPM4](https://github.com/openbmb/minicpm) ä¼˜åŒ–ã€‚
    - æ”¯æŒ InfLLM-v2 æ³¨æ„åŠ›å†…æ ¸
    - æ”¯æŒ MTP å±‚çš„æ»‘åŠ¨çª—å£ï¼Œä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡å¤„ç†
    - æ”¯æŒ MTP å±‚çš„é‡åŒ–
- [2025.05.29] æ”¯æŒ [SpecMQuant](https://github.com/AI9Stars/SpecMQuant) çš„é‡åŒ–ã€‚
    - æ”¯æŒ LLM çš„ Marlin GPTQ å†…æ ¸
    - æ”¯æŒé‡åŒ– LLM çš„æ¨æµ‹é‡‡æ ·
- [2025.03.01] åœ¨ [FR-Spec](https://github.com/thunlp/FR-Spec) å‘å¸ƒé¦–ä¸ªç‰ˆæœ¬ã€‚
    - æœ€å…ˆè¿›çš„æ¨æµ‹é‡‡æ ·å®ç°
    - æ”¯æŒ FR-Specï¼šåŸºäºé¢‘ç‡æ’åºçš„æ¨æµ‹é‡‡æ ·
    - æ”¯æŒ Flash-Attention ä¸­çš„æ ‘å½¢éªŒè¯
    - æ”¯æŒé™æ€å†…å­˜ç®¡ç†å’Œå†…å­˜é‡ç”¨
    - æ”¯æŒèåˆå†…æ ¸
    - æ”¯æŒåˆ†å—é¢„å¡«å……
    - æ”¯æŒ CUDA Graph

<div id="demo"></div>

## æ•ˆæœæ¼”ç¤º

![TODO å ä½ç¬¦](https://github.com/thunlp/Ouroboros/blob/main/figure/ouroboros.gif)

<div id="getstart"></div>

## å¿«é€Ÿå¼€å§‹

- [å®‰è£…](#install)
- [æ¨¡å‹æƒé‡](#modelweights)
- [è¿è¡Œç¤ºä¾‹](#example)

<div id="install"></div>

## å®‰è£…

### ä»æºç å®‰è£…

```bash
git clone https://github.com/OpenBMB/cpm.cu.git --recursive
cd cpm.cu
python3 setup.py install
```

<div id="modelweights"></div>

## å‡†å¤‡æ¨¡å‹

è¯·æŒ‰ç…§ [MiniCPM4](https://github.com/openbmb/minicpm) çš„è¯´æ˜ä¸‹è½½æ¨¡å‹æƒé‡ã€‚

<div id="example"></div>

## è¿è¡Œç¤ºä¾‹

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹æ¥å±•ç¤ºå¦‚ä½•ä½¿ç”¨ CPM.cuã€‚

```bash
python3 tests/test_generate.py
```

è¾“å‡ºåº”ä¸ºå¦‚ä¸‹æ ¼å¼ï¼š

```bash
Generated text (streaming output):
--------------------------------------------------
Prefilling: 100.0% (14774/14774 tokens) @ 6675.6 tokens/s - Complete!

<Generated Output HERE>
==================================================
Stream Generation Summary:
==================================================
Prefill length: 14774
Prefill time: 2.35 s
Prefill tokens/s: 6291.27
Decode length: 11
Decode time: 0.07 s
Decode tokens/s: 150.57
Mean accept length: 2.17
Decode token/s when acc = 1: 69.49
```

å…¶ä¸­ï¼š

- `Prefill` (è¾“å…¥) å’Œ `Decode` (è¾“å‡º) é€Ÿåº¦é€šè¿‡ï¼ˆé•¿åº¦ã€æ—¶é—´å’Œ token/sï¼‰è¾“å‡ºã€‚
- `Mean accept length` (å¹³å‡æ¥å—é•¿åº¦) æ˜¯ä½¿ç”¨æŠ•æœºé‡‡æ ·æ—¶æ¥å— token çš„å¹³å‡é•¿åº¦ã€‚
- `Decode token/s when acc = 1` æ˜¯å°†è¾“å‡ºé€Ÿåº¦é™¤ä»¥å¹³å‡æ¥å—é•¿åº¦çš„ç»“æœã€‚

## è‡´è°¢

æˆ‘ä»¬çš„ `src/flash_attn` æ–‡ä»¶å¤¹åŸºäº [FlashAttention](https://github.com/Dao-AILab/flash-attention/tree/v2.6.3/csrc/flash_attn) å¹¶è¿›è¡Œäº†ä¿®æ”¹ã€‚

æˆ‘ä»¬ä»ä»¥ä¸‹ä»“åº“ä¸­è·å–äº†å®ç°çµæ„Ÿï¼š

- [EAGLE](https://github.com/SafeAILab/EAGLE)
- [Block-Sparse-Attention](https://github.com/mit-han-lab/Block-Sparse-Attention)
- [vLLM](https://github.com/vllm-project/vllm)
- [SGLang](https://github.com/sgl-project/sglang)

## å¼•ç”¨

å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰ä»·å€¼ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ã€‚

```
@article{zhao2025fr,
  title={FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling},
  author={Zhao, Weilin and Pan, Tengyu and Han, Xu and Zhang, Yudi and Sun, Ao and Huang, Yuxiang and Zhang, Kaihuo and Zhao, Weilun and Li, Yuxuan and Wang, Jianyong and others},
  journal={arXiv preprint arXiv:2502.14856},
  year={2025}
}

@article{zhang2025specmqaunt,
  title={Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design},
  author={Zhang, Yudi and Zhao, Weilin and Han, Xu and Zhao, Tiejun and Xu, Wang and Cao, Hailong and Zhu, Conghui},
  journal={arXiv preprint arXiv:2505.22179},
  year={2025}
}

@article{minicpm4,
  title={MiniCPM4: Ultra-Efficient LLMs on End Devices},
  author={MiniCPM},
  year={2025}
}
```